{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Analysis \n",
    "\n",
    "Pavel Petruneac\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This is an analysis of the Steam Dataset. Data was provided in a small and larger version via GCS and more info about this dataset can be found [here](https://steam.internet.byu.edu/). \n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Data Engineering\n",
    "\n",
    "This exercise should be completed using PySpark (although feel free to use any of the APIs). Here is a guide on how to install PySpark on your local machine.\n",
    "\n",
    "1. Install and run PySpark.\n",
    "- Load .csv for Player_Summaries, Game_Publishers, Game_Genres, Game_Developers, Games_1 into PySpark dataframes.\n",
    "- Join all `Games_` tables into one dataframe.\n",
    "- Count the number of games per `publisher` and per `genre`.\n",
    "- Find day and hour when most new accounts were created (based on Player_Summaries table) e.g. 8pm on 14th August 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **TASK 1: Install and run PySpark**\n",
    "\n",
    "Check if `SparkSession` is loaded in the environment. `SparkSession` is already loaded in the environment via PySpark kernel, hence no need to import it like: \n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"steamAnalysis\")\\\n",
    "        .getOrCreate()\n",
    "spark   \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pp-dataproc-m.europe-west2-a.c.dyson-gds-dsdev.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe11018fdd8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.5 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Check Python version\n",
    "python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **TASK 2: Load .csv files into PySpark dataframes.**\n",
    "\n",
    "- Player_Summaries, \n",
    "- Game_Publishers, \n",
    "- Game_Genres,\n",
    "- Game_Developers, \n",
    "- Games_1 \n",
    "\n",
    "Data is saved in Google Cloud Storage bucket and it will be read straight into RAM. Before you run, make sure that you are authenticated with GCP. You can do it in a couple of ways: \n",
    "1. run `gcloud init` on your laptop terminal and follow instructions\n",
    "- if you run on GCP compute, allow the compute id read access to GCS bucket or \n",
    "- use a service account to authenticate on the terminal; can run something like \n",
    "\n",
    "    `gcloud auth activate-service-account *service_account_name* --key-file=credentials_file_path`, followed by \n",
    "    \n",
    "    `gcloud config set account *service_account_name*`\n",
    "    \n",
    "More info on authentication [here](https://cloud.google.com/sdk/gcloud/reference/auth/).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Authenticate to GCP with the service account + set the default account IF you run locally\n",
    "# # No need to do this if you run on GCP and have given GCS and BQ access to the compute ID. \n",
    "\n",
    "# import os \n",
    "\n",
    "\n",
    "# command = 'gcloud auth activate-service-account steam-analysis@north-star-213610.iam.gserviceaccount.com --key-file=../credentials/gcp_service_account.json'\n",
    "# with open('command.sh', 'w') as the_file:\n",
    "#   the_file.write(command)\n",
    "# # Copy files to GCS    \n",
    "# bashCommand = \"bash command.sh\"\n",
    "# os.system(bashCommand)\n",
    " \n",
    "# # Set default account \n",
    "# command = 'gcloud config set account steam-analysis@north-star-213610.iam.gserviceaccount.com'\n",
    "# with open('command.sh', 'w') as the_file:\n",
    "#   the_file.write(command)\n",
    "# # Copy files to GCS    \n",
    "# bashCommand = \"bash command.sh\"\n",
    "# os.system(bashCommand)\n",
    "\n",
    "# # Remove the command files\n",
    "# bashCommand = \"rm command.sh\"\n",
    "# os.system(bashCommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pp_steam_analyssis/data/sample/steam_gaming_large/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# List all the files in the bucket. \n",
    "# Comment dataset env var to list the small or large dataset. \n",
    "\n",
    "# Define env variables\n",
    "export gcs_bucket=\"pp_steam_analyssis\"\n",
    "# export dataset_type=\"steam_gaming_small\"\n",
    "export dataset=\"steam_gaming_large\"\n",
    "\n",
    "gsutil ls gs://$gcs_bucket/data/sample/$dataset_type/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will read steam_gaming_large dataset from GCS bucket: pp_steam_analyssis.\n"
     ]
    }
   ],
   "source": [
    "# Define a few global parameters \n",
    "gcs_bucket=\"pp_steam_analyssis\"\n",
    "\n",
    "# Define what dataset to read. True for small; False for large\n",
    "steam_gaming_small = False\n",
    "\n",
    "if steam_gaming_small:\n",
    "    dataset_type = \"steam_gaming_small\"\n",
    "else:\n",
    "    dataset_type = \"steam_gaming_large\"\n",
    "\n",
    "print(\"It will read {} dataset from GCS bucket: {}.\".format(dataset_type, gcs_bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** \n",
    "\n",
    "\n",
    "1. Before reading the data, check if there is a header, otherwise when using `header=True`, it will read the header as 1st row!\n",
    "2. `samplingRatio` is used as the ratio of the # of rows to use for inferring the schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,psteamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,psteamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,psteamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,psteamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,psteamid,personaname,profileurl,avatar,avatarmedium,avatarfull,personastate,communityvisibilitystate,p"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-100 gs://$gcs_bucket/data/sample/$dataset_type/Player_Summaries*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_summaries type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- steamid: long (nullable = true)\n",
      " |-- personaname: string (nullable = true)\n",
      " |-- profileurl: string (nullable = true)\n",
      " |-- avatar: string (nullable = true)\n",
      " |-- avatarmedium: string (nullable = true)\n",
      " |-- avatarfull: string (nullable = true)\n",
      " |-- personastate: string (nullable = true)\n",
      " |-- communityvisibilitystate: integer (nullable = true)\n",
      " |-- profilestate: integer (nullable = true)\n",
      " |-- lastlogoff: string (nullable = true)\n",
      " |-- commentpermission: string (nullable = true)\n",
      " |-- realname: string (nullable = true)\n",
      " |-- primaryclanid: string (nullable = true)\n",
      " |-- timecreated: string (nullable = true)\n",
      " |-- gameid: string (nullable = true)\n",
      " |-- gameserverip: string (nullable = true)\n",
      " |-- gameextrainfo: string (nullable = true)\n",
      " |-- cityid: string (nullable = true)\n",
      " |-- loccountrycode: string (nullable = true)\n",
      " |-- locstatecode: string (nullable = true)\n",
      " |-- loccityid: integer (nullable = true)\n",
      " |-- dateretrieved: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/Player_Summaries*.csv'.format(gcs_bucket, dataset_type)\n",
    "player_summaries = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"player_summaries type = {}\".format(type(player_summaries)))\n",
    "player_summaries.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appid,Publisher\r\n",
      "207990,\"\"\r\n",
      "215220,\"\"\r\n",
      "220824,\"\"\r\n",
      "24160"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-50 gs://$gcs_bucket/data/sample/$dataset_type/Games_Publishers*.csv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games_publishers type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/Games_Publishers*.csv'.format(gcs_bucket, dataset_type)\n",
    "games_publishers = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"games_publishers type = {}\".format(type(games_publishers)))\n",
    "games_publishers.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appid,Genre\r\n",
      "7290,RPG\r\n",
      "8980,RPG\r\n",
      "18010,RPG\r\n",
      "18040,RPG\r\n",
      "2"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-50 gs://$gcs_bucket/data/sample/$dataset_type/Games_Genres*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games_genres type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/Games_Genres*.csv'.format(gcs_bucket, dataset_type)\n",
    "games_genres = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"games_genres type = {}\".format(type(games_genres)))\n",
    "games_genres.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appid,Developer\r\n",
      "462530,8i\r\n",
      "452420,M2\r\n",
      "466530,M2\r\n",
      "36696"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-50 gs://$gcs_bucket/data/sample/$dataset_type/Games_Developers*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games_developers type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- Developer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/Games_Developers*.csv'.format(gcs_bucket, dataset_type)\n",
    "games_developers = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"games_developers type = {}\".format(type(games_developers)))\n",
    "games_developers.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steamid,appid,playtime_2weeks,playtime_forever,dateretrieved\n",
      "76561197972368092,55230,,2919,2013-05-13 06:11:48 UTC\n",
      "76561197972228702,42910,,18,2013-05-steamid,appid,playtime_2weeks,playtime_forever,dateretrieved\n",
      "76561198005175274,42690,,1312,2013-06-14 08:10:50 UTC\n",
      "76561198005175274,65730,84,108,2013-"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-150 gs://$gcs_bucket/data/sample/$dataset_type/Games_1*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games_1 type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- steamid: long (nullable = true)\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- playtime_2weeks: integer (nullable = true)\n",
      " |-- playtime_forever: integer (nullable = true)\n",
      " |-- dateretrieved: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/Games_1*.csv'.format(gcs_bucket, dataset_type)\n",
    "games_1 = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"games_1 type = {}\".format(type(games_1)))\n",
    "games_1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cat -r 0-150 gs://$gcs_bucket/data/sample/$dataset_type/Games_2*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_file_path = 'gs://{}/data/sample/{}/Games_2*.csv'.format(gcs_bucket, dataset_type)\n",
    "# games_2 = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "# print(\"games_2 type = {}\".format(type(games_2)))\n",
    "# games_2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **TASK 3: Join all `Games_` tables into one dataframe.**\n",
    "\n",
    "> Note: \n",
    "- the joining will be done on `appid` with join type = 'outer' to capture all data available. \n",
    "- the documentation states that `games_1` refers to initial crawl and `games_2` refers to follow-up crawl. In this join only initial crawl is considered. If needed, one can join the dataframes with `games_1.union(games_2)`\n",
    "\n",
    "**Steps:**\n",
    "1. join `games_1` (initial crawl) with \n",
    "- join `games_developers`\n",
    "- join `games_genres`\n",
    "- join `games_publishers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- steamid: long (nullable = true)\n",
      " |-- playtime_2weeks: integer (nullable = true)\n",
      " |-- playtime_forever: integer (nullable = true)\n",
      " |-- dateretrieved: string (nullable = true)\n",
      " |-- Developer: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "games = games_1.\\\n",
    "    join(games_developers, on='appid', how = 'outer').\\\n",
    "    join(games_genres, on='appid', how = 'outer').\\\n",
    "    join(games_publishers, on='appid', how = 'outer')\n",
    "\n",
    "\n",
    "print(\"games type = {}\".format(type(games)))\n",
    "games.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+---------------+----------------+--------------------+--------------+------+---------+\n",
      "|appid|          steamid|playtime_2weeks|playtime_forever|       dateretrieved|     Developer| Genre|Publisher|\n",
      "+-----+-----------------+---------------+----------------+--------------------+--------------+------+---------+\n",
      "| 4900|76561197970498542|           null|            3494|2013-05-11 23:09:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197970498542|           null|            3494|2013-05-11 23:09:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197972086370|           null|             225|2013-05-12 12:59:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197972086370|           null|             225|2013-05-12 12:59:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197971631392|           null|             204|2013-05-12 20:03:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197971631392|           null|             204|2013-05-12 20:03:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561198045735314|           null|             551|2013-08-10 16:33:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561198045735314|           null|             551|2013-08-10 16:33:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197973593062|           null|             122|2013-05-13 21:21:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197973593062|           null|             122|2013-05-13 21:21:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197970776304|           null|             687|2013-05-12 11:44:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197970776304|           null|             687|2013-05-12 11:44:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561198051740816|           null|            2509|2013-09-01 02:44:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561198051740816|           null|            2509|2013-09-01 02:44:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197971585476|           null|           21246|2013-05-13 03:13:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197971585476|           null|           21246|2013-05-13 03:13:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197993963646|           null|             105|2013-05-30 02:44:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197993963646|           null|             105|2013-05-30 02:44:...|Unknown Worlds|Casual|     null|\n",
      "| 4900|76561197970486502|           null|              22|2013-05-11 22:52:...|Unknown Worlds| Indie|     null|\n",
      "| 4900|76561197970486502|           null|              22|2013-05-11 22:52:...|Unknown Worlds|Casual|     null|\n",
      "+-----+-----------------+---------------+----------------+--------------------+--------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "games.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TASK 4: Count the number of games per `publisher` and per `genre`.**\n",
    "\n",
    "> Note:\n",
    "[Documentation](https://steam.internet.byu.edu/#) for *APP_ID_INFO* states that \n",
    "- *appid* is *The ID of the \"app\" in question, which is not necessarily a game* and \n",
    "- *type* The type of the \"app\". Possible values include: \"demo,\" \"dlc,\" \"game,\" \"hardware,\" \"mod,\" and \"video.\" Game is the most common.\n",
    "\n",
    "To get the count of games we need to merge the APP_ID_INFO dataframe with the games dataframe and then do a count, after filtering for the right app type (i.e. game). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appid,Title,Type,Price,Release_Date,Rating,Required_Age,Is_Multiplayer\r\n",
      "392230,Littlstar VR Cinema,game,0,1970-01-01 00:00:00 UTC,-1,0,0\r\n",
      "440000,Portal 2"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 0-150 gs://$gcs_bucket/data/sample/$dataset_type/App_ID_Info*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_ID_Info type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Release_Date: string (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Required_Age: integer (nullable = true)\n",
      " |-- Is_Multiplayer: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = 'gs://{}/data/sample/{}/App_ID_Info*.csv'.format(gcs_bucket, dataset_type)\n",
    "app_ID_Info = spark.read.csv(csv_file_path, header=True, inferSchema=True, samplingRatio = 0.05)\n",
    "\n",
    "print(\"app_ID_Info type = {}\".format(type(app_ID_Info)))\n",
    "app_ID_Info.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games_new type = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- appid: integer (nullable = true)\n",
      " |-- steamid: long (nullable = true)\n",
      " |-- playtime_2weeks: integer (nullable = true)\n",
      " |-- playtime_forever: integer (nullable = true)\n",
      " |-- dateretrieved: string (nullable = true)\n",
      " |-- Developer: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Release_Date: string (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Required_Age: integer (nullable = true)\n",
      " |-- Is_Multiplayer: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do a left join with games dataframe --> Only interested in the app types for which there is publisher and genre data. \n",
    "\n",
    "games_new = games.join(app_ID_Info, on ='appid', how='left')\n",
    "\n",
    "print(\"games_new type = {}\".format(type(games_new)))\n",
    "games_new.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the appid type distribution\n",
    "# games_new.select('Type').toPandas()['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+\n",
      "|           Publisher|       Genre|count(appid)|\n",
      "+--------------------+------------+------------+\n",
      "|               Valve|      Action|     4750004|\n",
      "|          Activision|      Action|     1341693|\n",
      "|         Aspyr (Mac)|      Action|     1150421|\n",
      "|                SEGA|    Strategy|      840062|\n",
      "|Feral Interactive...|    Strategy|      591212|\n",
      "|            2K Games|      Action|      553922|\n",
      "|            2K Games|    Strategy|      511185|\n",
      "|               Valve|Free to Play|      416418|\n",
      "|         Square Enix|      Action|      401992|\n",
      "|Feral Interactive...|    Strategy|      377856|\n",
      "|Feral Interactive...|      Action|      356213|\n",
      "|      Rockstar Games|      Action|      322508|\n",
      "|         Deep Silver|      Action|      296274|\n",
      "|     Electronic Arts|      Action|      289271|\n",
      "|             Ubisoft|      Action|      282748|\n",
      "|               Valve|    Strategy|      276377|\n",
      "|            2K Games|         RPG|      272382|\n",
      "|  Bethesda Softworks|         RPG|      266782|\n",
      "|        Nordic Games|      Action|      257195|\n",
      "|  Aspyr (Mac, Linux)|    Strategy|      237320|\n",
      "|Warner Bros. Inte...|      Action|      235118|\n",
      "|         Square Enix|   Adventure|      204085|\n",
      "|                SEGA|      Action|      198152|\n",
      "|Feral Interactive...|   Adventure|      173922|\n",
      "|      Rockstar Games|   Adventure|      168724|\n",
      "|      Telltale Games|   Adventure|      161732|\n",
      "|             Ubisoft|   Adventure|      150668|\n",
      "| Bohemia Interactive|      Action|      149973|\n",
      "| Bohemia Interactive|  Simulation|      147769|\n",
      "| Bohemia Interactive|    Strategy|      147733|\n",
      "+--------------------+------------+------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for the 'game' appid type\n",
    "counts = games_new.filter(games_new.Type == 'game').\\\n",
    "groupBy('Publisher', 'Genre').\\\n",
    "agg({'appid':'count'})\n",
    "\n",
    "counts.orderBy(counts['count(appid)'].desc()).show(30) # order desc by the counts and show top 30 results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.S. You can also ge the results into a pandas dataframe: counts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TASK 5: Find day and hour when most new accounts were created**\n",
    "\n",
    "> (based on Player_Summaries table) e.g. 8pm on 14th August 2005.\n",
    "\n",
    "> Note:\n",
    "To get the the day and hour when most accounts were created, we need to use the `timecreated` field. Will extract day and hour and do a groupby, followed by count. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- steamid: long (nullable = true)\n",
      " |-- personaname: string (nullable = true)\n",
      " |-- profileurl: string (nullable = true)\n",
      " |-- avatar: string (nullable = true)\n",
      " |-- avatarmedium: string (nullable = true)\n",
      " |-- avatarfull: string (nullable = true)\n",
      " |-- personastate: string (nullable = true)\n",
      " |-- communityvisibilitystate: integer (nullable = true)\n",
      " |-- profilestate: integer (nullable = true)\n",
      " |-- lastlogoff: string (nullable = true)\n",
      " |-- commentpermission: string (nullable = true)\n",
      " |-- realname: string (nullable = true)\n",
      " |-- primaryclanid: string (nullable = true)\n",
      " |-- timecreated: string (nullable = true)\n",
      " |-- gameid: string (nullable = true)\n",
      " |-- gameserverip: string (nullable = true)\n",
      " |-- gameextrainfo: string (nullable = true)\n",
      " |-- cityid: string (nullable = true)\n",
      " |-- loccountrycode: string (nullable = true)\n",
      " |-- locstatecode: string (nullable = true)\n",
      " |-- loccityid: integer (nullable = true)\n",
      " |-- dateretrieved: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A refresher on the schema available \n",
    "player_summaries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        timecreated|\n",
      "+-------------------+\n",
      "|2012-02-23 08:11:02|\n",
      "|2005-01-29 12:11:10|\n",
      "|2007-08-27 13:38:33|\n",
      "|2012-04-03 10:51:59|\n",
      "|2004-11-17 07:57:04|\n",
      "|2004-12-26 06:35:19|\n",
      "|2012-01-05 20:55:33|\n",
      "|2011-10-26 10:28:57|\n",
      "|2004-12-31 16:45:39|\n",
      "|2011-06-08 00:52:18|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See a few values in timecreated\n",
    "player_summaries.select('timecreated').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "|month|day|hour|\n",
      "+-----+---+----+\n",
      "|    2| 23|   8|\n",
      "|    1| 29|  12|\n",
      "|    8| 27|  13|\n",
      "|    4|  3|  10|\n",
      "|   11| 17|   7|\n",
      "|   12| 26|   6|\n",
      "|    1|  5|  20|\n",
      "|   10| 26|  10|\n",
      "|   12| 31|  16|\n",
      "|    6|  8|   0|\n",
      "+-----+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract month, day of the month and hour ++ Drop null values. \n",
    "from pyspark.sql.functions import month, dayofmonth, hour\n",
    "\n",
    "month_day_hour = player_summaries.select(\n",
    "    month(\"timecreated\").alias('month'), \n",
    "    dayofmonth(\"timecreated\").alias('day'),\n",
    "    hour(\"timecreated\").alias('hour'), \n",
    ").dropna() \n",
    "\n",
    "month_day_hour.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----------+\n",
      "|month|day|hour|count(day)|\n",
      "+-----+---+----+----------+\n",
      "|   12| 25|  10|      2933|\n",
      "|   12| 25|   8|      2811|\n",
      "|   12| 25|  11|      2620|\n",
      "|   12| 25|   7|      2438|\n",
      "|    3|  2|  10|      2390|\n",
      "|   12| 25|  12|      2363|\n",
      "|   12| 25|   3|      2268|\n",
      "|   12| 25|   6|      2261|\n",
      "|   12| 25|   9|      2196|\n",
      "|   12| 25|   5|      2153|\n",
      "+-----+---+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = month_day_hour.groupBy('month', 'day', 'hour').\\\n",
    "    agg({'day':'count'})\n",
    "\n",
    "counts.orderBy(counts['count(day)'].desc()).show(10) # order desc by the counts and show top 10 results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The results above show that most of the accounts were created on the 25th of December at 10am - surprinsingly on Christmas Day :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.3)",
   "language": "python",
   "name": "pythonwithpixiedustspark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
