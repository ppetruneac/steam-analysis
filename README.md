 # Steam Gaming Platform - data analysis 

  This project consists of [Steam dataset](https://steam.internet.byu.edu/dataset) analysis. To see the challenge requirements, please see the [Challenge](./Challenge.md) file. Please see the Project Organisation section below for description of file structure. There aren't many files, but this structure was used for consistency. 

  
  
  The main analysis is saved in the `/notebooks` folder as *.ipynb* files and there is a rendered version of the notebooks saved in *.html* extensions. Please refer to this folder to evaluate Exercise 1-3. The work is documented in the notebooks. The `/src` folder contains empty files generated by the folder structure and the `reports/figures` contains screenshots of the analysis in Exercise 2.  

  `/credentials` folder is for GCP service account, but the exercises were done on GCP with the required product permissions, hence the service account was not needed, but in some cases code for service account authentication was provided. 


  ## Before you start

  If you want to replicate the analysis, please: 
  1. install pySpark on your system - used in Exercise 1.
  2. (optional) install [anaconda](https://www.anaconda.com/) for Jupyter server and conda package manager. 
  3. install dependancies with `pip install -r requirements.txt`. 


  ## Exercises Overview

  1. Exercise 1: focuses on data engineering topics - mainly loading and merging data with pySpark. Please refer to [steam_exercise_1](./notebooks/steam_exercise_1.html). 
  2. Exercise 2: a data analysis to back up a business case. Data was pushed to GCP BQ and analysis was done in Datastudio. Plese read [steam_exercise_2](./notebooks/steam_exercise_2.html) for data loading, preprocessing, assumptions and view the dashboard here: [Steam Analysis - Mental Health ?](https://datastudio.google.com/u/0/reporting/1umbIL-FNX9H6ssKgL59VJ-lrVkeWXLCt/page/piHr) (access may be terminated, but screenshots are provided in this repo in [reports/figures/](./reports/figures/)) folder.

  > Please note that field names could not be renamed even though the right permission to data source was granted. This may have to do with the fact that a personal GCP account was used as opposed to a corporate one. 

  3. Exercise 3: aims for something novel such as a deep dive for analytics or some machine learning. Building on the knowledge from step 2,  [steam_exercise_3](./notebooks/steam_exercise_3.html) presents a model that predicts the user play time given the number of friends the user is connected to, the number of groups the user has joined and the platform tenure in terms of months after sign up. Three models are explored and compared and recommendations for other ideas are provided at the end. 


  
  ## Project Organization
  ------------
    |__ credentials        <- folder for credentials files. 
    |
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── interim        <- Intermediate data that has been transformed.
    │   └── sample         <- A sample of the original data.
    │
    ├── docs               <- Project Documentation that includes busines problem, assumtions, data wrangling & exploration, 
    │                         model methodology, development and implementation.
    │
    ├── logs               <- Logs directory
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Notebooks (i.e. 'notebook.xyz')
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── dockerfile         <- File to automate building a Docker container off the code in src, dependencies, and notebooks
    │
    ├── src                <- Source code for use in this project.
       ├── main.xyz        <- Main code to run. 
       │
       ├── functions       <- Scripts to download/generate data; build features, train models or visualize data. 
          |
          └── make_dataset.xyz
          └── build_features.xyz
          └── train.xyz
          └── visualize.xyz

  Project structure inspired by [cookiecutter](https://cookiecutter.readthedocs.io/en/latest/).

  

  
  